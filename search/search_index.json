{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started \u00b6 Documentation in Progress Check back soon for more updates. Cloning the Repository \u00b6 Start by cloning or downloading this repository to your local computer. You can clone this repository using the Github Desktop Client or using the Git command line. Clone using SSH git clone git@github.com:bellevue-university/dsc650.git Clone using HTTPS git clone https://github.com/bellevue-university/dsc650.git You will need access to this repository throughout the course, so place it in a reliable location. After you are finished cloning or downloading the repository, you will open the repository directory using PyCharm. You can do this by opening PyCharm and selecting the File -> Open option and choosing the directory containing the repository. Download PyCharm if you do not already have it installed on your computer. You can also download PyCharm for Anaconda to install PyCharm with the Anaconda Python interpreter. Setup \u00b6 Windows macOS Ubuntu","title":"Getting Started"},{"location":"#getting-started","text":"Documentation in Progress Check back soon for more updates.","title":"Getting Started"},{"location":"#cloning-the-repository","text":"Start by cloning or downloading this repository to your local computer. You can clone this repository using the Github Desktop Client or using the Git command line. Clone using SSH git clone git@github.com:bellevue-university/dsc650.git Clone using HTTPS git clone https://github.com/bellevue-university/dsc650.git You will need access to this repository throughout the course, so place it in a reliable location. After you are finished cloning or downloading the repository, you will open the repository directory using PyCharm. You can do this by opening PyCharm and selecting the File -> Open option and choosing the directory containing the repository. Download PyCharm if you do not already have it installed on your computer. You can also download PyCharm for Anaconda to install PyCharm with the Anaconda Python interpreter.","title":"Cloning the Repository"},{"location":"#setup","text":"Windows macOS Ubuntu","title":"Setup"},{"location":"data-sources/","text":"Data Sources \u00b6 This repository contains data from multiple sources. These sources released made this data available under permissive licenses including the Creative Commons . This data is used for educational purposes only. 538 The New York Times R for Data Science Tidynomicon Network Data Sets OpenFlights","title":"Data Sources"},{"location":"data-sources/#data-sources","text":"This repository contains data from multiple sources. These sources released made this data available under permissive licenses including the Creative Commons . This data is used for educational purposes only. 538 The New York Times R for Data Science Tidynomicon Network Data Sets OpenFlights","title":"Data Sources"},{"location":"data/coco/","text":"COCO \u00b6 The folder data/external/coco contains data from the Common Objects in Context COCO COCO website . Terms of Use \u00b6 Annotations & Website The annotations in this dataset along with this website belong to the COCO Consortium and are licensed under a Creative Commons Attribution 4.0 License. Images The COCO Consortium does not own the copyright of the images. Use of the images must abide by the Flickr Terms of Use. The users of the images accept full responsibility for the use of the dataset, including but not limited to the use of any copies of copyrighted images that they may create from the dataset. Software Copyright \u00a9 2015, COCO Consortium. All rights reserved. Redistribution and use software in source and binary form, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the COCO Consortium nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE AND ANNOTATIONS ARE PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"COCO"},{"location":"data/coco/#coco","text":"The folder data/external/coco contains data from the Common Objects in Context COCO COCO website .","title":"COCO"},{"location":"data/coco/#terms-of-use","text":"Annotations & Website The annotations in this dataset along with this website belong to the COCO Consortium and are licensed under a Creative Commons Attribution 4.0 License. Images The COCO Consortium does not own the copyright of the images. Use of the images must abide by the Flickr Terms of Use. The users of the images accept full responsibility for the use of the dataset, including but not limited to the use of any copies of copyrighted images that they may create from the dataset. Software Copyright \u00a9 2015, COCO Consortium. All rights reserved. Redistribution and use software in source and binary form, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the COCO Consortium nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE AND ANNOTATIONS ARE PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"Terms of Use"},{"location":"data/enron/","text":"Enron Emails \u00b6","title":"Enron Emails"},{"location":"data/enron/#enron-emails","text":"","title":"Enron Emails"},{"location":"data/jester/","text":"Jester \u00b6 The folder data/external/jester contains data from the Jester dataset .","title":"Jester"},{"location":"data/jester/#jester","text":"The folder data/external/jester contains data from the Jester dataset .","title":"Jester"},{"location":"data/movielens/","text":"MovieLens \u00b6 The data/external/movielens folder contains the ml-latest-small dataset downloaded from MovieLens . License \u00b6 Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions: The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group. The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information). The user may redistribute the data set, including transformations, so long as it is distributed under these same license conditions. The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota. The executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. The entire risk as to the quality and performance of them is with you. Should the program prove defective, you assume the cost of all necessary servicing, repair or correction. In no event shall the University of Minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).","title":"MovieLens"},{"location":"data/movielens/#movielens","text":"The data/external/movielens folder contains the ml-latest-small dataset downloaded from MovieLens .","title":"MovieLens"},{"location":"data/movielens/#license","text":"Neither the University of Minnesota nor any of the researchers involved can guarantee the correctness of the data, its suitability for any particular purpose, or the validity of results based on the use of the data set. The data set may be used for any research purposes under the following conditions: The user may not state or imply any endorsement from the University of Minnesota or the GroupLens Research Group. The user must acknowledge the use of the data set in publications resulting from the use of the data set (see below for citation information). The user may redistribute the data set, including transformations, so long as it is distributed under these same license conditions. The user may not use this information for any commercial or revenue-bearing purposes without first obtaining permission from a faculty member of the GroupLens Research Project at the University of Minnesota. The executable software scripts are provided \"as is\" without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose. The entire risk as to the quality and performance of them is with you. Should the program prove defective, you assume the cost of all necessary servicing, repair or correction. In no event shall the University of Minnesota, its affiliates or employees be liable to you for any damages arising out of the use or inability to use these programs (including but not limited to loss of data or data being rendered inaccurate).","title":"License"},{"location":"data/nytimes-covid/","text":"NY Times COVID-19 \u00b6 License and Attribution \u00b6 In general, we are making this data publicly available for broad, noncommercial public use including by medical and public health researchers, policymakers, analysts and local news media. If you use this data, you must attribute it to \u201cThe New York Times\u201d in any publication. If you would like a more expanded description of the data, you could say \u201cData from The New York Times, based on reports from state and local health agencies.\u201d If you use it in an online presentation, we would appreciate it if you would link to our U.S. tracking page at https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html . If you use this data, please let us know at covid-data@nytimes.com . See our LICENSE for the full terms of use for this data. This license is co-extensive with the Creative Commons Attribution-NonCommercial 4.0 International license, and licensees should refer to that license ( CC BY-NC ) if they have questions about the scope of the license.","title":"NY Times COVID-19"},{"location":"data/nytimes-covid/#ny-times-covid-19","text":"","title":"NY Times COVID-19"},{"location":"data/nytimes-covid/#license-and-attribution","text":"In general, we are making this data publicly available for broad, noncommercial public use including by medical and public health researchers, policymakers, analysts and local news media. If you use this data, you must attribute it to \u201cThe New York Times\u201d in any publication. If you would like a more expanded description of the data, you could say \u201cData from The New York Times, based on reports from state and local health agencies.\u201d If you use it in an online presentation, we would appreciate it if you would link to our U.S. tracking page at https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html . If you use this data, please let us know at covid-data@nytimes.com . See our LICENSE for the full terms of use for this data. This license is co-extensive with the Creative Commons Attribution-NonCommercial 4.0 International license, and licensees should refer to that license ( CC BY-NC ) if they have questions about the scope of the license.","title":"License and Attribution"},{"location":"data/openflights/","text":"OpenFlights \u00b6 License \u00b6 The OpenFlights Airport, Airline, Plane and Route Databases are made available under the Open Database License . Any rights in individual contents of the database are licensed under the Database Contents License . In short, these mean that you are welcome to use the data as you wish, if and only if you both acknowledge the source and and license any derived works made available to the public with a free license as well. See OpenFlights Data for more detailed documentation. Data \u00b6 OpenFlights data is found in data/external/openflights . Data copied from the OpenFlights Github Repo . Note The special value \\N is used for \\\"NULL\\\" to indicate that no value is available Airports \u00b6 Field Type Nullable? Notes airport_id int No Primary Key name text Yes city text Yes country text Yes iata varchar(3) Yes icao varchar(4) Yes latitude double No longitude double No altitude int Yes timezone float Yes dst char(1) Yes tz_id text Yes type text Yes source text Yes Airlines \u00b6 Field Type Nullable? Notes airline_id int No Primary Key name text No alias text Yes iata varchar(2) Yes icao varchar(3) Yes callsign text Yes country text Yes active boolean No Default value FALSE Routes \u00b6 Field Type Nullable? Notes airline varchar(3) Yes airline_id int Yes src_airport varchar(4) Yes src_airport_id int Yes dst_airport varchar(4) Yes dst_airport_id int Yes codeshare boolean Yes Default value FALSE stops int Yes equipment text Yes airline_id , src_airport_id , and dst_airport_id form a unique key Planes \u00b6 Field Type Nullable? Notes name text Yes iata varchar(3) Yes icao varchar(4) Yes Countries \u00b6 Field Type Nullable? Notes name text Yes iso_code varchar(2) Yes dafif_code varchar(2) Yes Note Some entries have DAFIF codes, but not ISO codes. These are primarily uninhabited islands without airports, and can be ignored for most purposes.","title":"OpenFlights"},{"location":"data/openflights/#openflights","text":"","title":"OpenFlights"},{"location":"data/openflights/#license","text":"The OpenFlights Airport, Airline, Plane and Route Databases are made available under the Open Database License . Any rights in individual contents of the database are licensed under the Database Contents License . In short, these mean that you are welcome to use the data as you wish, if and only if you both acknowledge the source and and license any derived works made available to the public with a free license as well. See OpenFlights Data for more detailed documentation.","title":"License"},{"location":"data/openflights/#data","text":"OpenFlights data is found in data/external/openflights . Data copied from the OpenFlights Github Repo . Note The special value \\N is used for \\\"NULL\\\" to indicate that no value is available","title":"Data"},{"location":"data/openflights/#airports","text":"Field Type Nullable? Notes airport_id int No Primary Key name text Yes city text Yes country text Yes iata varchar(3) Yes icao varchar(4) Yes latitude double No longitude double No altitude int Yes timezone float Yes dst char(1) Yes tz_id text Yes type text Yes source text Yes","title":"Airports"},{"location":"data/openflights/#airlines","text":"Field Type Nullable? Notes airline_id int No Primary Key name text No alias text Yes iata varchar(2) Yes icao varchar(3) Yes callsign text Yes country text Yes active boolean No Default value FALSE","title":"Airlines"},{"location":"data/openflights/#routes","text":"Field Type Nullable? Notes airline varchar(3) Yes airline_id int Yes src_airport varchar(4) Yes src_airport_id int Yes dst_airport varchar(4) Yes dst_airport_id int Yes codeshare boolean Yes Default value FALSE stops int Yes equipment text Yes airline_id , src_airport_id , and dst_airport_id form a unique key","title":"Routes"},{"location":"data/openflights/#planes","text":"Field Type Nullable? Notes name text Yes iata varchar(3) Yes icao varchar(4) Yes","title":"Planes"},{"location":"data/openflights/#countries","text":"Field Type Nullable? Notes name text Yes iso_code varchar(2) Yes dafif_code varchar(2) Yes Note Some entries have DAFIF codes, but not ISO codes. These are primarily uninhabited islands without airports, and can be ignored for most purposes.","title":"Countries"},{"location":"data/openfood/","text":"Open Food Facts \u00b6 Open Food Facts Data License \u00b6 The Open Food Facts database is available under the Open Database License . The individual contents of the database are available under the Database Contents License . Products images are available under the Creative Commons Attribution ShareAlike licence . They may contain graphical elements subject to copyright or other rights, that may in some cases be reproduced (quotation rights or fair use). Please read Terms and conditions of use and re-use before re-using the data.","title":"Open Food Facts"},{"location":"data/openfood/#open-food-facts","text":"Open Food Facts Data","title":"Open Food Facts"},{"location":"data/openfood/#license","text":"The Open Food Facts database is available under the Open Database License . The individual contents of the database are available under the Database Contents License . Products images are available under the Creative Commons Attribution ShareAlike licence . They may contain graphical elements subject to copyright or other rights, that may in some cases be reproduced (quotation rights or fair use). Please read Terms and conditions of use and re-use before re-using the data.","title":"License"},{"location":"data/tidynomicon/","text":"Tidynomicon \u00b6 Data copied from the Tidynomicon Github repository . License \u00b6 This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by/4.0/legalcode for the full legal text. This work is licensed under the Creative Commons Attribution 4.0 International license (CC-BY-4.0). You are free to: Share ---copy and redistribute the material in any medium or format Remix ---remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution ---You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions ---You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Tidynomicon"},{"location":"data/tidynomicon/#tidynomicon","text":"Data copied from the Tidynomicon Github repository .","title":"Tidynomicon"},{"location":"data/tidynomicon/#license","text":"This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by/4.0/legalcode for the full legal text. This work is licensed under the Creative Commons Attribution 4.0 International license (CC-BY-4.0). You are free to: Share ---copy and redistribute the material in any medium or format Remix ---remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution ---You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions ---You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"License"},{"location":"data/wikilens/","text":"WikiLens \u00b6 The folder data/external/wikilens contains data from the WikiLens dataset.","title":"WikiLens"},{"location":"data/wikilens/#wikilens","text":"The folder data/external/wikilens contains data from the WikiLens dataset.","title":"WikiLens"},{"location":"lessons/","text":"Lessons \u00b6 Big Data Foundations Deep Learning","title":"Lessons"},{"location":"lessons/#lessons","text":"Big Data Foundations Deep Learning","title":"Lessons"},{"location":"lessons/deep-learning/","text":"Deep Learning \u00b6 Introduction","title":"Deep Learning"},{"location":"lessons/deep-learning/#deep-learning","text":"Introduction","title":"Deep Learning"},{"location":"lessons/deep-learning/introduction/","text":"","title":"Introduction"},{"location":"lessons/foundations/","text":"Foundations of Big Data \u00b6 Introduction Performance Latency Data Size","title":"Foundations of Big Data"},{"location":"lessons/foundations/#foundations-of-big-data","text":"Introduction Performance Latency Data Size","title":"Foundations of Big Data"},{"location":"lessons/foundations/big-data/","text":"Introduction \u00b6 Volume, variety, and velocity \u00b6 Data Models \u00b6 Relational Graph Document","title":"Introduction"},{"location":"lessons/foundations/big-data/#introduction","text":"","title":"Introduction"},{"location":"lessons/foundations/big-data/#volume-variety-and-velocity","text":"","title":"Volume, variety, and velocity"},{"location":"lessons/foundations/big-data/#data-models","text":"Relational Graph Document","title":"Data Models"},{"location":"lessons/foundations/latency/","text":"Latency \u00b6 Latency Numbers Every Data Scientist Should Know In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know . These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon . Below is yet another update on these numbers with data taken from Colin Scott , a Berkeley researcher. An interactive version of this repository can be found here . Action Latency (ns) 1 Latency (\u03bc) 2 Latency (ms) 3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA->Netherlands->CA 150,000,000 ns 150,000 \u03bcs 150 ms Notes 1 ns = 10 -9 seconds \u21a9 1 \u03bcs = 10 -6 seconds = 1,000 ns \u21a9 1 ms = 10 -3 seconds = 1,000 \u03bcs = 1,000,000 ns \u21a9","title":"Latency"},{"location":"lessons/foundations/latency/#latency","text":"Latency Numbers Every Data Scientist Should Know In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know . These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon . Below is yet another update on these numbers with data taken from Colin Scott , a Berkeley researcher. An interactive version of this repository can be found here . Action Latency (ns) 1 Latency (\u03bc) 2 Latency (ms) 3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA->Netherlands->CA 150,000,000 ns 150,000 \u03bcs 150 ms Notes 1 ns = 10 -9 seconds \u21a9 1 \u03bcs = 10 -6 seconds = 1,000 ns \u21a9 1 ms = 10 -3 seconds = 1,000 \u03bcs = 1,000,000 ns \u21a9","title":"Latency"},{"location":"lessons/foundations/performance/","text":"Performance \u00b6 Year Country Vendor Computer Performance Units 1938 Germany Zuse Z1 1 IPS 1941 Germany Zuse Z3 20 IPS 1946 United States University of Pennsylvania ENIAC 5 kIPS 1951 United States Massachusetts Institute of Technology Whirlwind I 20 kIPS 1958 United States IBM AN/FSQ-7 75 kIPS 1960 United States IBM 7090 229 kIPS 1960 United States UNIVAC LARC 250 kIPS 1961 United States IBM 7030 Stretch 1.2 MIPS 1962 United Kingdom University of Manchester Atlas 1 MFLOPS 1964 United States CDC 6600 3 MFLOPS 1969 United States CDC 7600 36 MFLOPS 1974 United States CDC STAR-100 100 MFLOPS 1976 United States Cray Cray-1 160 MFLOPS 1983 United States Cray X-MP/4 713 MFLOPS 1985 United States Cray Cray-2 1.41 GFLOPS 1988 United States Cray Y-MP/832 2.14 GFLOPS 1990 Japan Fujitsu VP2600/10 4 GFLOPS 1992 Japan NEC SX-3/44 20 GFLOPS 1993 United States Thinking CM-5/1024 59.7 GFLOPS 1993 Japan Fujitsu Numerical Wind Tunnel 124.2 GFLOPS 1994 United States Intel Paragon XP/S 140 143.4 GFLOPS 1994 Japan Fujitsu Numerical Wind Tunnel 170 GFLOPS 1996 Japan Hitachi SR2201 232.4 GFLOPS 1996 Japan Hitachi CP-PACS 368.2 GFLOPS 1997 United States Intel ASCI Red 1.06 TFLOPS 2000 United States IBM ASCI White 4.93 TFLOPS 2001 United States IBM ASCI White 7.2 TFLOPS 2002 Japan NEC Earth Simulator 35.86 TFLOPS 2004 United States IBM Blue Gene/L 70.72 TFLOPS 2005 United States IBM Blue Gene/L 136.8 TFLOPS 2005 United States IBM Blue Gene/L 280.6 TFLOPS 2007 United States IBM Blue Gene/L 478.2 TFLOPS 2008 United States IBM Roadrunner 1.02 PFLOPS 2008 United States IBM Roadrunner 1.1 PFLOPS 2009 United States Cray Jaguar 1.75 PFLOPS 2010 China National University of Defense Technology Tianhe-1A 2.57 PFLOPS 2011 Japan Fujitsu K computer 10.51 PFLOPS 2012 United States IBM Sequoia (Blue Gene/Q) 16.32 PFLOPS 2012 United States Cray Titan 17.59 PFLOPS 2013 China National University of Defense Technology Tianhe-2 33.86 PFLOPS 2016 China NRCPC Sunway TaihuLight 93.01 PFLOPS 2018 United States IBM Summit 122.3 PFLOPS 2019 United States IBM Summit 148.6 PFLOPS CPU 239494208 Floating pointing operations per second 537003968 Integer operations per second 1771991 Hash operations GPU 52 FPS 1845 GFLOPS RAM 21717 MB/s Disk Write 466 MB/s Read 1908 MB/s https://en.wikipedia.org/wiki/List_of_fastest_computers https://en.wikipedia.org/wiki/Summit_(supercomputer )","title":"Performance"},{"location":"lessons/foundations/performance/#performance","text":"Year Country Vendor Computer Performance Units 1938 Germany Zuse Z1 1 IPS 1941 Germany Zuse Z3 20 IPS 1946 United States University of Pennsylvania ENIAC 5 kIPS 1951 United States Massachusetts Institute of Technology Whirlwind I 20 kIPS 1958 United States IBM AN/FSQ-7 75 kIPS 1960 United States IBM 7090 229 kIPS 1960 United States UNIVAC LARC 250 kIPS 1961 United States IBM 7030 Stretch 1.2 MIPS 1962 United Kingdom University of Manchester Atlas 1 MFLOPS 1964 United States CDC 6600 3 MFLOPS 1969 United States CDC 7600 36 MFLOPS 1974 United States CDC STAR-100 100 MFLOPS 1976 United States Cray Cray-1 160 MFLOPS 1983 United States Cray X-MP/4 713 MFLOPS 1985 United States Cray Cray-2 1.41 GFLOPS 1988 United States Cray Y-MP/832 2.14 GFLOPS 1990 Japan Fujitsu VP2600/10 4 GFLOPS 1992 Japan NEC SX-3/44 20 GFLOPS 1993 United States Thinking CM-5/1024 59.7 GFLOPS 1993 Japan Fujitsu Numerical Wind Tunnel 124.2 GFLOPS 1994 United States Intel Paragon XP/S 140 143.4 GFLOPS 1994 Japan Fujitsu Numerical Wind Tunnel 170 GFLOPS 1996 Japan Hitachi SR2201 232.4 GFLOPS 1996 Japan Hitachi CP-PACS 368.2 GFLOPS 1997 United States Intel ASCI Red 1.06 TFLOPS 2000 United States IBM ASCI White 4.93 TFLOPS 2001 United States IBM ASCI White 7.2 TFLOPS 2002 Japan NEC Earth Simulator 35.86 TFLOPS 2004 United States IBM Blue Gene/L 70.72 TFLOPS 2005 United States IBM Blue Gene/L 136.8 TFLOPS 2005 United States IBM Blue Gene/L 280.6 TFLOPS 2007 United States IBM Blue Gene/L 478.2 TFLOPS 2008 United States IBM Roadrunner 1.02 PFLOPS 2008 United States IBM Roadrunner 1.1 PFLOPS 2009 United States Cray Jaguar 1.75 PFLOPS 2010 China National University of Defense Technology Tianhe-1A 2.57 PFLOPS 2011 Japan Fujitsu K computer 10.51 PFLOPS 2012 United States IBM Sequoia (Blue Gene/Q) 16.32 PFLOPS 2012 United States Cray Titan 17.59 PFLOPS 2013 China National University of Defense Technology Tianhe-2 33.86 PFLOPS 2016 China NRCPC Sunway TaihuLight 93.01 PFLOPS 2018 United States IBM Summit 122.3 PFLOPS 2019 United States IBM Summit 148.6 PFLOPS CPU 239494208 Floating pointing operations per second 537003968 Integer operations per second 1771991 Hash operations GPU 52 FPS 1845 GFLOPS RAM 21717 MB/s Disk Write 466 MB/s Read 1908 MB/s https://en.wikipedia.org/wiki/List_of_fastest_computers https://en.wikipedia.org/wiki/Summit_(supercomputer )","title":"Performance"},{"location":"lessons/foundations/size/","text":"Data Size \u00b6 Metric Value Unit Symbol Notes 1 1 Byte B 1 byte = 1 letter in computer memory 10^{3} 10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6} 10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9} 10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12} 10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15} 10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18} 10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21} 10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24} 10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth Binary Value Unit Symbol Notes 2^{10} 2^{10} Kibibyte Ki 1024 bytes 2^{20} 2^{20} Mebibyte Mi 1024 kibibytes 2^{30} 2^{30} Gibibyte Gi 1024 mebibytes 2^{40} 2^{40} Tebibyte Ti 1024 gibibytes 2^{50} 2^{50} Pebibyte Pi 1024 tebibytes 2^{60} 2^{60} Exbibyte Ei 1024 pebibytes 2^{70} 2^{70} Zebibyte Zi 1024 exbibytes 2^{80} 2^{80} Yobibyte Yi 1024 zebibytes","title":"Data Size"},{"location":"lessons/foundations/size/#data-size","text":"Metric Value Unit Symbol Notes 1 1 Byte B 1 byte = 1 letter in computer memory 10^{3} 10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6} 10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9} 10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12} 10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15} 10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18} 10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21} 10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24} 10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth Binary Value Unit Symbol Notes 2^{10} 2^{10} Kibibyte Ki 1024 bytes 2^{20} 2^{20} Mebibyte Mi 1024 kibibytes 2^{30} 2^{30} Gibibyte Gi 1024 mebibytes 2^{40} 2^{40} Tebibyte Ti 1024 gibibytes 2^{50} 2^{50} Pebibyte Pi 1024 tebibytes 2^{60} 2^{60} Exbibyte Ei 1024 pebibytes 2^{70} 2^{70} Zebibyte Zi 1024 exbibytes 2^{80} 2^{80} Yobibyte Yi 1024 zebibytes","title":"Data Size"},{"location":"lessons/foundations/systems/","text":"Data Systems \u00b6 Streaming Batch Processing","title":"Data Systems"},{"location":"lessons/foundations/systems/#data-systems","text":"Streaming Batch Processing","title":"Data Systems"},{"location":"lessons/fundamentals/latency-numbers/","text":"Latency Numbers Every Data Scientist Should Know \u00b6 In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know . These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon . Below is yet another update on these numbers with data taken from Colin Scott , a Berkeley researcher. An interactive version of this repository can be found here . Action Latency (ns) 1 Latency (\u03bc) 2 Latency (ms) 3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA->Netherlands->CA 150,000,000 ns 150,000 \u03bcs 150 ms Notes 1 ns = 10 -9 seconds \u21a9 1 \u03bcs = 10 -6 seconds = 1,000 ns \u21a9 1 ms = 10 -3 seconds = 1,000 \u03bcs = 1,000,000 ns \u21a9","title":"Latency Numbers"},{"location":"lessons/fundamentals/latency-numbers/#latency-numbers-every-data-scientist-should-know","text":"In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know . These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon . Below is yet another update on these numbers with data taken from Colin Scott , a Berkeley researcher. An interactive version of this repository can be found here . Action Latency (ns) 1 Latency (\u03bc) 2 Latency (ms) 3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA->Netherlands->CA 150,000,000 ns 150,000 \u03bcs 150 ms Notes 1 ns = 10 -9 seconds \u21a9 1 \u03bcs = 10 -6 seconds = 1,000 ns \u21a9 1 ms = 10 -3 seconds = 1,000 \u03bcs = 1,000,000 ns \u21a9","title":"Latency Numbers Every Data Scientist Should Know"},{"location":"lessons/fundamentals/three-vs/","text":"Volume, velocity, and variety \u00b6","title":"Volume, velocity, and variety"},{"location":"lessons/fundamentals/three-vs/#volume-velocity-and-variety","text":"","title":"Volume, velocity, and variety"},{"location":"lessons/fundamentals/units/","text":"Units of Measure \u00b6 Data Size \u00b6 Metric Value Unit Symbol Notes 1 1 Byte B 1 byte = 1 letter in computer memory 10^{3} 10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6} 10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9} 10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12} 10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15} 10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18} 10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21} 10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24} 10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth Binary Value Unit Symbol Notes 2^{10} 2^{10} Kibibyte Ki 1024 bytes 2^{20} 2^{20} Mebibyte Mi 1024 kibibytes 2^{30} 2^{30} Gibibyte Gi 1024 mebibytes 2^{40} 2^{40} Tebibyte Ti 1024 gibibytes 2^{50} 2^{50} Pebibyte Pi 1024 tebibytes 2^{60} 2^{60} Exbibyte Ei 1024 pebibytes 2^{70} 2^{70} Zebibyte Zi 1024 exbibytes 2^{80} 2^{80} Yobibyte Yi 1024 zebibytes","title":"Units"},{"location":"lessons/fundamentals/units/#units-of-measure","text":"","title":"Units of Measure"},{"location":"lessons/fundamentals/units/#data-size","text":"Metric Value Unit Symbol Notes 1 1 Byte B 1 byte = 1 letter in computer memory 10^{3} 10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6} 10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9} 10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12} 10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15} 10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18} 10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21} 10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24} 10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth Binary Value Unit Symbol Notes 2^{10} 2^{10} Kibibyte Ki 1024 bytes 2^{20} 2^{20} Mebibyte Mi 1024 kibibytes 2^{30} 2^{30} Gibibyte Gi 1024 mebibytes 2^{40} 2^{40} Tebibyte Ti 1024 gibibytes 2^{50} 2^{50} Pebibyte Pi 1024 tebibytes 2^{60} 2^{60} Exbibyte Ei 1024 pebibytes 2^{70} 2^{70} Zebibyte Zi 1024 exbibytes 2^{80} 2^{80} Yobibyte Yi 1024 zebibytes","title":"Data Size"},{"location":"lessons/storage/","text":"Data Storage \u00b6 Data Encoding Data Structures","title":"Data Storage"},{"location":"lessons/storage/#data-storage","text":"Data Encoding Data Structures","title":"Data Storage"},{"location":"lessons/storage/complexity/","text":"","title":"Complexity"},{"location":"lessons/storage/encoding/","text":"Data Encoding \u00b6","title":"Data Encoding"},{"location":"lessons/storage/encoding/#data-encoding","text":"","title":"Data Encoding"},{"location":"lessons/storage/models/","text":"Data Models \u00b6 NoSQL Relational \u00b6 Graph \u00b6 Document \u00b6","title":"Data Models"},{"location":"lessons/storage/models/#data-models","text":"NoSQL","title":"Data Models"},{"location":"lessons/storage/models/#relational","text":"","title":"Relational"},{"location":"lessons/storage/models/#graph","text":"","title":"Graph"},{"location":"lessons/storage/models/#document","text":"","title":"Document"},{"location":"lessons/storage/queries/","text":"Query Processing \u00b6 Graph \u00b6 Relational \u00b6","title":"Query Processing"},{"location":"lessons/storage/queries/#query-processing","text":"","title":"Query Processing"},{"location":"lessons/storage/queries/#graph","text":"","title":"Graph"},{"location":"lessons/storage/queries/#relational","text":"","title":"Relational"},{"location":"lessons/storage/structures/","text":"","title":"Structures"},{"location":"setup/","text":"Setup \u00b6 Windows macOS Ubuntu","title":"Setup"},{"location":"setup/#setup","text":"Windows macOS Ubuntu","title":"Setup"},{"location":"setup/macOS/","text":"macOS \u00b6 Documentation in Progress Check back soon for more updates.","title":"macOS"},{"location":"setup/macOS/#macos","text":"Documentation in Progress Check back soon for more updates.","title":"macOS"},{"location":"setup/ubuntu/","text":"Ubuntu \u00b6 Documentation in Progress Check back soon for more updates.","title":"Ubuntu"},{"location":"setup/ubuntu/#ubuntu","text":"Documentation in Progress Check back soon for more updates.","title":"Ubuntu"},{"location":"setup/windows/","text":"Windows \u00b6 Documentation in Progress Check back soon for more updates.","title":"Windows"},{"location":"setup/windows/#windows","text":"Documentation in Progress Check back soon for more updates.","title":"Windows"},{"location":"setup/in-progress/macOS/","text":"macOS \u00b6 Anaconda \u00b6 Install Anaconda PyCharm \u00b6 Install PyCharm Project Interpreter \u00b6 File -> Settings Project -> Project Interpreter Change Terminal \u00b6 File -> Settings Tools -> Terminal JDK \u00b6 Install JDK 8 https://www.oracle.com/java/technologies/javase-jdk8-downloads.html Add to ~/.bash_profile or ~/.zshrc . export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home Homebrew \u00b6 Open terminal Install Homebrew /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" Update Package brew update Install Spark brew install apache-arrow brew install apache-spark brew install avro-tools brew install git brew install hadoop brew install pandoc brew install pandoc-citeproc brew install pandoc-crossref brew install parquet-tools brew install protobuf brew install snappy brew install thrift PySpark \u00b6 Keras and TensorFlow \u00b6","title":"macOS"},{"location":"setup/in-progress/macOS/#macos","text":"","title":"macOS"},{"location":"setup/in-progress/macOS/#anaconda","text":"Install Anaconda","title":"Anaconda"},{"location":"setup/in-progress/macOS/#pycharm","text":"Install PyCharm","title":"PyCharm"},{"location":"setup/in-progress/macOS/#project-interpreter","text":"File -> Settings Project -> Project Interpreter","title":"Project Interpreter"},{"location":"setup/in-progress/macOS/#change-terminal","text":"File -> Settings Tools -> Terminal","title":"Change Terminal"},{"location":"setup/in-progress/macOS/#jdk","text":"Install JDK 8 https://www.oracle.com/java/technologies/javase-jdk8-downloads.html Add to ~/.bash_profile or ~/.zshrc . export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home","title":"JDK"},{"location":"setup/in-progress/macOS/#homebrew","text":"Open terminal Install Homebrew /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" Update Package brew update Install Spark brew install apache-arrow brew install apache-spark brew install avro-tools brew install git brew install hadoop brew install pandoc brew install pandoc-citeproc brew install pandoc-crossref brew install parquet-tools brew install protobuf brew install snappy brew install thrift","title":"Homebrew"},{"location":"setup/in-progress/macOS/#pyspark","text":"","title":"PySpark"},{"location":"setup/in-progress/macOS/#keras-and-tensorflow","text":"","title":"Keras and TensorFlow"},{"location":"setup/in-progress/packages/","text":"Python \u00b6 pyarrow pyspark OS-Specific Packages \u00b6 Package Chocolatey Homebrew Avro Tools N/A avro-tools etcd etcd etcd Kafka kafka kafka Spark N/A apache-spark Zookeeper apache-zookeeper zookeeper Chocolatey \u00b6 choco install apache-zookeeper Brew \u00b6 brew install zookeeper","title":"Packages"},{"location":"setup/in-progress/packages/#python","text":"pyarrow pyspark","title":"Python"},{"location":"setup/in-progress/packages/#os-specific-packages","text":"Package Chocolatey Homebrew Avro Tools N/A avro-tools etcd etcd etcd Kafka kafka kafka Spark N/A apache-spark Zookeeper apache-zookeeper zookeeper","title":"OS-Specific Packages"},{"location":"setup/in-progress/packages/#chocolatey","text":"choco install apache-zookeeper","title":"Chocolatey"},{"location":"setup/in-progress/packages/#brew","text":"brew install zookeeper","title":"Brew"},{"location":"setup/in-progress/ubuntu/","text":"Ubuntu \u00b6 Anaconda \u00b6 Install Anaconda PyCharm \u00b6 Install PyCharm Project Interpreter \u00b6 File -> Settings Project -> Project Interpreter Change Terminal \u00b6 File -> Settings Tools -> Terminal JDK \u00b6 Install JDK 8 https://www.oracle.com/java/technologies/javase-jdk8-downloads.html Add to ~/.bash_profile or ~/.zshrc . export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home Homebrew \u00b6 Open terminal Install Homebrew /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" Update Package brew update Install Spark brew install apache-arrow brew install apache-spark brew install avro-tools brew install git brew install hadoop brew install pandoc brew install pandoc-citeproc brew install pandoc-crossref brew install parquet-tools brew install protobuf brew install snappy brew install thrift PySpark \u00b6 Keras and TensorFlow \u00b6","title":"Ubuntu"},{"location":"setup/in-progress/ubuntu/#ubuntu","text":"","title":"Ubuntu"},{"location":"setup/in-progress/ubuntu/#anaconda","text":"Install Anaconda","title":"Anaconda"},{"location":"setup/in-progress/ubuntu/#pycharm","text":"Install PyCharm","title":"PyCharm"},{"location":"setup/in-progress/ubuntu/#project-interpreter","text":"File -> Settings Project -> Project Interpreter","title":"Project Interpreter"},{"location":"setup/in-progress/ubuntu/#change-terminal","text":"File -> Settings Tools -> Terminal","title":"Change Terminal"},{"location":"setup/in-progress/ubuntu/#jdk","text":"Install JDK 8 https://www.oracle.com/java/technologies/javase-jdk8-downloads.html Add to ~/.bash_profile or ~/.zshrc . export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home","title":"JDK"},{"location":"setup/in-progress/ubuntu/#homebrew","text":"Open terminal Install Homebrew /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" Update Package brew update Install Spark brew install apache-arrow brew install apache-spark brew install avro-tools brew install git brew install hadoop brew install pandoc brew install pandoc-citeproc brew install pandoc-crossref brew install parquet-tools brew install protobuf brew install snappy brew install thrift","title":"Homebrew"},{"location":"setup/in-progress/ubuntu/#pyspark","text":"","title":"PySpark"},{"location":"setup/in-progress/ubuntu/#keras-and-tensorflow","text":"","title":"Keras and TensorFlow"},{"location":"setup/in-progress/windows/","text":"Windows \u00b6 Windows Linux Sub-System \u00b6 Chocolatey \u00b6 https://chocolatey.org/ https://chocolatey.org/install Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1')) choco install hadoop choco install jdk8 choco install scala Using admin privileges install the following choco install anaconda3 choco install git choco install github-desktop choco install hadoop choco install jdk8 choco install parq choco install protoc choco install thrift JetBrains choco install datagrip choco install jetbrainstoolbox choco install pycharm choco install pycharm-community choco install pycharm-edu Optional choco install graphviz choco install hugo choco install miktex choco install pandoc choco install pandoc-crossref choco install postgresql choco install vagrant choco install vagrant-manager choco install virtualbox Anaconda \u00b6 PyCharm \u00b6 Project Interpreter \u00b6 File -> Settings Project -> Project Interpreter Change Terminal \u00b6 File -> Settings Tools -> Terminal Hadoop \u00b6 Using admin privileges install the following choco install hadoop Download Winutils Copy 2.7.1 bin to hadoop-3.2.1/bin PySpark \u00b6 Download Spark http://spark.apache.org/downloads.html 2.4.5 release, pre-built for Hadoop 2.7 https://towardsdatascience.com/installing-apache-pyspark-on-windows-10-f5f0c506bea1 https://github.com/steveloughran/winutils Environment Variables \u00b6 Variable Value HADOOP_HOME C:\\Hadoop\\hadoop-3.2.1 JAVA_HOME C:\\Program Files\\Java\\jdk1.8.0_211 SPARK_HOME C:\\Spark PS C: \\U sers \\s hermans \\D ocuments \\G itHub \\d sc650> cd . \\e xamples \\ PS C: \\U sers \\s hermans \\D ocuments \\G itHub \\d sc650 \\e xamples> python . \\p i.py > pi.txt 20 /05/07 07 :14:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark ' s default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\" . To adjust logging level use sc.setLogLevel ( newLevel ) . For SparkR, use setLogLevel ( newLevel ) . And the result Pi is roughly 3 .137360 SUCCESS: The process with PID 11928 ( child process of PID 16116 ) has been terminated. SUCCESS: The process with PID 16116 ( child process of PID 7812 ) has been terminated. SUCCESS: The process with PID 7812 ( child process of PID 16368 ) has been terminated. Keras and TensorFlow \u00b6 PS C: \\U sers \\s hermans \\D ocuments \\G itHub \\d sc650 \\e xamples> python . \\m nist_mlp.py > mnist_mlp.txt 2020 -05-07 07 :13:52.076079: W tensorflow/stream_executor/platform/default/dso_loader.cc:55 ] Could not load dynamic library 'cudart64_101.dll' ; dlerror: cudart64_101.dll not found 2020 -05-07 07 :13:52.081494: I tensorflow/stream_executor/cuda/cudart_stub.cc:29 ] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2020 -05-07 07 :13:54.926282: W tensorflow/stream_executor/platform/default/dso_loader.cc:55 ] Could not load dynamic library 'nvcuda.dll' ; dlerror: nvcuda.dll not found 2020 -05-07 07 :13:54.938885: E tensorflow/stream_executor/cuda/cuda_driver.cc:351 ] failed call to cuInit: UNKNOWN ERROR ( 303 ) 2020 -05-07 07 :13:54.948587: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169 ] retrieving CUDA diagnostic information for host: DESKTOP-A669FJG 2020 -05-07 07 :13:54.957037: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176 ] hostname: DESKTOP-A669FJG 2020 -05-07 07 :13:54.961726: I tensorflow/core/platform/cpu_feature_guard.cc:142 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2","title":"Windows"},{"location":"setup/in-progress/windows/#windows","text":"","title":"Windows"},{"location":"setup/in-progress/windows/#windows-linux-sub-system","text":"","title":"Windows Linux Sub-System"},{"location":"setup/in-progress/windows/#chocolatey","text":"https://chocolatey.org/ https://chocolatey.org/install Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1')) choco install hadoop choco install jdk8 choco install scala Using admin privileges install the following choco install anaconda3 choco install git choco install github-desktop choco install hadoop choco install jdk8 choco install parq choco install protoc choco install thrift JetBrains choco install datagrip choco install jetbrainstoolbox choco install pycharm choco install pycharm-community choco install pycharm-edu Optional choco install graphviz choco install hugo choco install miktex choco install pandoc choco install pandoc-crossref choco install postgresql choco install vagrant choco install vagrant-manager choco install virtualbox","title":"Chocolatey"},{"location":"setup/in-progress/windows/#anaconda","text":"","title":"Anaconda"},{"location":"setup/in-progress/windows/#pycharm","text":"","title":"PyCharm"},{"location":"setup/in-progress/windows/#project-interpreter","text":"File -> Settings Project -> Project Interpreter","title":"Project Interpreter"},{"location":"setup/in-progress/windows/#change-terminal","text":"File -> Settings Tools -> Terminal","title":"Change Terminal"},{"location":"setup/in-progress/windows/#hadoop","text":"Using admin privileges install the following choco install hadoop Download Winutils Copy 2.7.1 bin to hadoop-3.2.1/bin","title":"Hadoop"},{"location":"setup/in-progress/windows/#pyspark","text":"Download Spark http://spark.apache.org/downloads.html 2.4.5 release, pre-built for Hadoop 2.7 https://towardsdatascience.com/installing-apache-pyspark-on-windows-10-f5f0c506bea1 https://github.com/steveloughran/winutils","title":"PySpark"},{"location":"setup/in-progress/windows/#environment-variables","text":"Variable Value HADOOP_HOME C:\\Hadoop\\hadoop-3.2.1 JAVA_HOME C:\\Program Files\\Java\\jdk1.8.0_211 SPARK_HOME C:\\Spark PS C: \\U sers \\s hermans \\D ocuments \\G itHub \\d sc650> cd . \\e xamples \\ PS C: \\U sers \\s hermans \\D ocuments \\G itHub \\d sc650 \\e xamples> python . \\p i.py > pi.txt 20 /05/07 07 :14:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark ' s default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to \"WARN\" . To adjust logging level use sc.setLogLevel ( newLevel ) . For SparkR, use setLogLevel ( newLevel ) . And the result Pi is roughly 3 .137360 SUCCESS: The process with PID 11928 ( child process of PID 16116 ) has been terminated. SUCCESS: The process with PID 16116 ( child process of PID 7812 ) has been terminated. SUCCESS: The process with PID 7812 ( child process of PID 16368 ) has been terminated.","title":"Environment Variables"},{"location":"setup/in-progress/windows/#keras-and-tensorflow","text":"PS C: \\U sers \\s hermans \\D ocuments \\G itHub \\d sc650 \\e xamples> python . \\m nist_mlp.py > mnist_mlp.txt 2020 -05-07 07 :13:52.076079: W tensorflow/stream_executor/platform/default/dso_loader.cc:55 ] Could not load dynamic library 'cudart64_101.dll' ; dlerror: cudart64_101.dll not found 2020 -05-07 07 :13:52.081494: I tensorflow/stream_executor/cuda/cudart_stub.cc:29 ] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 2020 -05-07 07 :13:54.926282: W tensorflow/stream_executor/platform/default/dso_loader.cc:55 ] Could not load dynamic library 'nvcuda.dll' ; dlerror: nvcuda.dll not found 2020 -05-07 07 :13:54.938885: E tensorflow/stream_executor/cuda/cuda_driver.cc:351 ] failed call to cuInit: UNKNOWN ERROR ( 303 ) 2020 -05-07 07 :13:54.948587: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169 ] retrieving CUDA diagnostic information for host: DESKTOP-A669FJG 2020 -05-07 07 :13:54.957037: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176 ] hostname: DESKTOP-A669FJG 2020 -05-07 07 :13:54.961726: I tensorflow/core/platform/cpu_feature_guard.cc:142 ] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2","title":"Keras and TensorFlow"}]}